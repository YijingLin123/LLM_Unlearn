import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import sys

MODEL_PATH = "./fatpatch_output_classifier_arxiv"  # ä½ ä¿å­˜çš„æ¨¡å‹ç›®å½•
MAX_LENGTH = 512

# âœ… åŠ è½½æ¨¡å‹å’Œ tokenizer
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
model = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH).to(device)
model.eval()

# âœ… æ¨ç†å‡½æ•°
def classify_text(text, threshold=0.5):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding="max_length", max_length=MAX_LENGTH).to(device)
    with torch.no_grad():
        outputs = model(**inputs)
        probs = torch.softmax(outputs.logits, dim=-1).squeeze()
        label = int(probs[1] > threshold)
    return label, probs[1].item()

if __name__ == "__main__":
    text = input("è¯·è¾“å…¥ä½ æƒ³åˆ¤æ–­çš„æ–‡æœ¬ï¼š\n> ")
    label, score = classify_text(text)
    print(f"\nğŸ” æ˜¯å¦å‘½ä¸­ Forget åŒºåŸŸ: {'âœ… æ˜¯' if label else 'âŒ å¦'}ï¼ˆå¾—åˆ†: {score:.4f}ï¼‰")
