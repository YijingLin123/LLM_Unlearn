arXiv

gradient ascent

{'loss': -18.0593, 'grad_norm': 4.138686656951904, 'learning_rate': 2.0000000000000003e-06, 'epoch': 1.0}
{'train_runtime': 186.8908, 'train_samples_per_second': 2.675, 'train_steps_per_second': 0.268, 'train_loss': -20.276553955078125, 'epoch': 1.0}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [03:06<00:00,  3.74s/it]
ðŸš¨ 1 epoch åŽ LoRA adapter æ¬§æ°è·ç¦»: 3.339436
æ¨¡åž‹å·²ä¿å­˜åˆ°GA_pou_proof/model_final

Target steps            : [36, 38, 40, 42, 44, 46]
Original distances      : ['0.2540', '0.3604', '0.4440', '0.5147', '0.5749', '0.6260']
Reproduced distances    : ['0.5363', '0.6145', '0.6712', '0.7287', '0.7892', '0.8541']
ðŸ“Š Pearson correlation r: 0.9938
âœ… è¶‹åŠ¿ä¸€è‡´æ€§è‰¯å¥½ï¼ˆé«˜ç›¸å…³æ€§ï¼‰

approximate retrain

{'loss': 127.5213, 'grad_norm': nan, 'learning_rate': 1.980487804878049e-05, 'epoch': 0.02}
{'loss': 122.7418, 'grad_norm': 38.934295654296875, 'learning_rate': 1.9512195121951222e-05, 'epoch': 0.03}
{'loss': 121.1296, 'grad_norm': 48.58367156982422, 'learning_rate': 1.91869918699187e-05, 'epoch': 0.05}
{'loss': 116.4495, 'grad_norm': 57.5338249206543, 'learning_rate': 1.886178861788618e-05, 'epoch': 0.06}
{'loss': 112.7313, 'grad_norm': 70.06859588623047, 'learning_rate': 1.8536585365853663e-05, 'epoch': 0.08}
{'loss': 107.5912, 'grad_norm': 82.03514862060547, 'learning_rate': 1.821138211382114e-05, 'epoch': 0.1}
{'loss': 103.9761, 'grad_norm': 109.80516052246094, 'learning_rate': 1.788617886178862e-05, 'epoch': 0.11}
{'loss': 97.9048, 'grad_norm': 173.35304260253906, 'learning_rate': 1.75609756097561e-05, 'epoch': 0.13}
{'loss': 89.6496, 'grad_norm': 146.81985473632812, 'learning_rate': 1.7235772357723578e-05, 'epoch': 0.15}
{'loss': 80.474, 'grad_norm': 170.57630920410156, 'learning_rate': 1.691056910569106e-05, 'epoch': 0.16}
{'loss': 69.4674, 'grad_norm': 161.9749298095703, 'learning_rate': 1.6585365853658537e-05, 'epoch': 0.18}
{'loss': 59.4392, 'grad_norm': 193.69093322753906, 'learning_rate': 1.6260162601626018e-05, 'epoch': 0.19}
{'loss': 49.5458, 'grad_norm': 240.4566650390625, 'learning_rate': 1.5934959349593496e-05, 'epoch': 0.21}
{'loss': 36.7198, 'grad_norm': 200.7642364501953, 'learning_rate': 1.5609756097560978e-05, 'epoch': 0.23}
{'loss': 27.4235, 'grad_norm': 152.86231994628906, 'learning_rate': 1.528455284552846e-05, 'epoch': 0.24}
{'loss': 16.5583, 'grad_norm': 162.9923095703125, 'learning_rate': 1.4959349593495935e-05, 'epoch': 0.26}
{'loss': 11.8382, 'grad_norm': 124.44113159179688, 'learning_rate': 1.4666666666666666e-05, 'epoch': 0.28}
{'loss': 6.9928, 'grad_norm': 70.4526138305664, 'learning_rate': 1.4341463414634148e-05, 'epoch': 0.29}
{'loss': 1.7516, 'grad_norm': 28.919963836669922, 'learning_rate': 1.4016260162601627e-05, 'epoch': 0.31}
{'loss': 0.8171, 'grad_norm': 16.451122283935547, 'learning_rate': 1.3691056910569107e-05, 'epoch': 0.32}
{'loss': 0.4197, 'grad_norm': 7.479043006896973, 'learning_rate': 1.3365853658536587e-05, 'epoch': 0.34}
{'loss': 2.0746, 'grad_norm': 7.697150230407715, 'learning_rate': 1.307317073170732e-05, 'epoch': 0.36}
{'loss': 1.6481, 'grad_norm': 12.1893310546875, 'learning_rate': 1.2747967479674799e-05, 'epoch': 0.37}
{'loss': 0.1885, 'grad_norm': 11.589627265930176, 'learning_rate': 1.2422764227642277e-05, 'epoch': 0.39}
{'loss': 0.3491, 'grad_norm': 1.8093845844268799, 'learning_rate': 1.2097560975609757e-05, 'epoch': 0.41}
{'loss': 0.1375, 'grad_norm': 2.526588201522827, 'learning_rate': 1.1772357723577236e-05, 'epoch': 0.42}
{'loss': 1.884, 'grad_norm': 2.5247275829315186, 'learning_rate': 1.1447154471544718e-05, 'epoch': 0.44}
{'loss': 0.1363, 'grad_norm': 3.6979639530181885, 'learning_rate': 1.1121951219512197e-05, 'epoch': 0.45}
{'loss': 0.1203, 'grad_norm': 2.787895441055298, 'learning_rate': 1.0796747967479675e-05, 'epoch': 0.47}
{'loss': 0.1142, 'grad_norm': 2.4139890670776367, 'learning_rate': 1.0471544715447155e-05, 'epoch': 0.49}
{'loss': 0.1091, 'grad_norm': 3.8432071208953857, 'learning_rate': 1.0146341463414634e-05, 'epoch': 0.5}
{'loss': 1.9276, 'grad_norm': 2.3691952228546143, 'learning_rate': 9.821138211382114e-06, 'epoch': 0.52}
{'loss': 0.1049, 'grad_norm': 2.371029853820801, 'learning_rate': 9.495934959349594e-06, 'epoch': 0.54}
{'loss': 0.1064, 'grad_norm': 3.6346936225891113, 'learning_rate': 9.170731707317075e-06, 'epoch': 0.55}
{'loss': 0.1051, 'grad_norm': 1.9522247314453125, 'learning_rate': 8.845528455284553e-06, 'epoch': 0.57}
{'loss': 1.0394, 'grad_norm': 1.3987891674041748, 'learning_rate': 8.520325203252033e-06, 'epoch': 0.58}
{'loss': 0.1027, 'grad_norm': 0.7375861406326294, 'learning_rate': 8.195121951219512e-06, 'epoch': 0.6}
{'loss': 1.11, 'grad_norm': 0.87907475233078, 'learning_rate': 7.869918699186992e-06, 'epoch': 0.62}
{'loss': 0.0918, 'grad_norm': 1.3347867727279663, 'learning_rate': 7.544715447154472e-06, 'epoch': 0.63}
{'loss': 2.6393, 'grad_norm': 163.486572265625, 'learning_rate': 7.219512195121952e-06, 'epoch': 0.65}
{'loss': 2.0203, 'grad_norm': 0.521376371383667, 'learning_rate': 6.894308943089432e-06, 'epoch': 0.67}
{'loss': 0.0969, 'grad_norm': 3.7265377044677734, 'learning_rate': 6.56910569105691e-06, 'epoch': 0.68}
{'loss': 0.0951, 'grad_norm': 4.016565322875977, 'learning_rate': 6.243902439024391e-06, 'epoch': 0.7}
{'loss': 0.1011, 'grad_norm': 8.953370094299316, 'learning_rate': 5.9186991869918705e-06, 'epoch': 0.71}
{'loss': 0.1142, 'grad_norm': 0.9493617415428162, 'learning_rate': 5.59349593495935e-06, 'epoch': 0.73}
{'loss': 0.094, 'grad_norm': 2.4705910682678223, 'learning_rate': 5.26829268292683e-06, 'epoch': 0.75}
{'loss': 2.5503, 'grad_norm': 0.7143458724021912, 'learning_rate': 4.943089430894309e-06, 'epoch': 0.76}
{'loss': 2.2794, 'grad_norm': 0.6605238318443298, 'learning_rate': 4.617886178861789e-06, 'epoch': 0.78}
{'loss': 0.0931, 'grad_norm': 1.720812201499939, 'learning_rate': 4.292682926829269e-06, 'epoch': 0.8}
{'loss': 0.0865, 'grad_norm': 0.6470783948898315, 'learning_rate': 3.967479674796748e-06, 'epoch': 0.81}
{'loss': 3.3117, 'grad_norm': 0.42555510997772217, 'learning_rate': 3.6422764227642283e-06, 'epoch': 0.83}
{'loss': 0.7782, 'grad_norm': 1.0838552713394165, 'learning_rate': 3.3170731707317076e-06, 'epoch': 0.84}
{'loss': 0.0878, 'grad_norm': 0.5763441920280457, 'learning_rate': 2.991869918699187e-06, 'epoch': 0.86}
{'loss': 1.0661, 'grad_norm': 2.4195315837860107, 'learning_rate': 2.666666666666667e-06, 'epoch': 0.88}
{'loss': 2.3405, 'grad_norm': 65.5363540649414, 'learning_rate': 2.3414634146341465e-06, 'epoch': 0.89}
{'loss': 0.0881, 'grad_norm': 1.8118730783462524, 'learning_rate': 2.016260162601626e-06, 'epoch': 0.91}
{'loss': 2.9674, 'grad_norm': 0.4206673204898834, 'learning_rate': 1.723577235772358e-06, 'epoch': 0.93}
{'loss': 0.092, 'grad_norm': 1.467395544052124, 'learning_rate': 1.3983739837398375e-06, 'epoch': 0.94}
{'loss': 0.0833, 'grad_norm': 3.0722036361694336, 'learning_rate': 1.0731707317073172e-06, 'epoch': 0.96}
{'loss': 0.0891, 'grad_norm': 1.583742380142212, 'learning_rate': 7.479674796747968e-07, 'epoch': 0.97}
{'loss': 3.55, 'grad_norm': 0.59588623046875, 'learning_rate': 4.2276422764227643e-07, 'epoch': 0.99}
{'train_runtime': 2263.8799, 'train_samples_per_second': 2.719, 'train_steps_per_second': 0.272, 'train_loss': 22.71801047804879, 'epoch': 1.0}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 615/615 [37:43<00:00,  3.68s/it]
ðŸš¨ 1 epoch åŽ LoRA adapter æ¬§æ°è·ç¦»: 39.845344
æ¨¡åž‹å·²ä¿å­˜åˆ°AR_pou_proof/model_final

ðŸ“ˆ [Trend Verification]
Target steps            : [604, 606, 608, 610, 612, 614]
Original distances      : ['0.0141', '0.0172', '0.0196', '0.0218', '0.0238', '0.0254']
Reproduced distances    : ['0.7768', '0.8664', '0.8887', '0.9859', '1.0679', '1.1066']
ðŸ“Š Pearson correlation r: 0.9861
âœ… è¶‹åŠ¿ä¸€è‡´æ€§è‰¯å¥½ï¼ˆé«˜ç›¸å…³æ€§ï¼‰

Finetune with random labels

{'loss': 223.5002, 'grad_norm': 43.67233657836914, 'learning_rate': 1.76e-05, 'epoch': 0.2}
{'loss': 223.4253, 'grad_norm': 54.070796966552734, 'learning_rate': 1.4e-05, 'epoch': 0.4}
{'loss': 214.0728, 'grad_norm': 51.38497543334961, 'learning_rate': 1e-05, 'epoch': 0.6}
{'loss': 214.2106, 'grad_norm': 47.816226959228516, 'learning_rate': 6e-06, 'epoch': 0.8}
{'loss': 216.8787, 'grad_norm': 70.60037231445312, 'learning_rate': 2.0000000000000003e-06, 'epoch': 1.0}
{'train_runtime': 184.9041, 'train_samples_per_second': 2.704, 'train_steps_per_second': 0.27, 'train_loss': 218.4175146484375, 'epoch': 1.0}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [03:04<00:00,  3.70s/it]
ðŸš¨ 1 epoch åŽ LoRA adapter æ¬§æ°è·ç¦»: 6.470628
æ¨¡åž‹å·²ä¿å­˜åˆ°FT_pou_proof/model_final

ðŸ“ˆ [Trend Verification]
Target steps            : [36, 38, 40, 42, 44, 46]
Original distances      : ['0.4709', '0.6710', '0.8522', '1.0114', '1.1486', '1.2617']
Reproduced distances    : ['0.5112', '0.6024', '0.6811', '0.8476', '0.9596', '1.0844']
ðŸ“Š Pearson correlation r: 0.9834
âœ… è¶‹åŠ¿ä¸€è‡´æ€§è‰¯å¥½ï¼ˆé«˜ç›¸å…³æ€§ï¼‰

unlearn with adversarial samples TODO

{'loss': 8.0686, 'grad_norm': 6.784513473510742, 'learning_rate': 1.6800000000000002e-05, 'epoch': 0.2}
{'loss': 8.0933, 'grad_norm': 5.540737628936768, 'learning_rate': 1.3600000000000002e-05, 'epoch': 0.4}
{'loss': 8.0437, 'grad_norm': 2.968644857406616, 'learning_rate': 1e-05, 'epoch': 0.6}
{'loss': 7.8334, 'grad_norm': 3.619595766067505, 'learning_rate': 6.4000000000000006e-06, 'epoch': 0.8}
{'loss': 7.8924, 'grad_norm': 46.792694091796875, 'learning_rate': 2.4000000000000003e-06, 'epoch': 1.0}
{'train_runtime': 298.9066, 'train_samples_per_second': 1.673, 'train_steps_per_second': 0.167, 'train_loss': 7.986246490478516, 'epoch': 1.0}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [04:58<00:00,  5.98s/it]
ðŸš¨ 1 epoch åŽ LoRA adapter æ¬§æ°è·ç¦»: 4.928576
æ¨¡åž‹å·²ä¿å­˜åˆ°UA_pou_proof/model_final

ðŸ“ˆ [Trend Verification]
Target steps            : [36, 38, 40, 42, 44, 46]
Original distances      : ['0.3590', '0.4425', '0.5993', '0.7392', '0.8603', '0.9598']
Reproduced distances    : ['0.5798', '0.7158', '0.8095', '0.8987', '0.9834', '1.0773']
ðŸ“Š Pearson correlation r: 0.9909
âœ… è¶‹åŠ¿ä¸€è‡´æ€§è‰¯å¥½ï¼ˆé«˜ç›¸å…³æ€§ï¼‰

gradient ascent + descent

{'loss': 18.6725, 'grad_norm': 2.927389621734619, 'learning_rate': 1.936e-05, 'epoch': 0.04}
{'loss': 18.2161, 'grad_norm': 2.5253896713256836, 'learning_rate': 1.864e-05, 'epoch': 0.08}
{'loss': 21.2431, 'grad_norm': 2.7351632118225098, 'learning_rate': 1.8e-05, 'epoch': 0.12}
{'loss': 19.4898, 'grad_norm': 2.799473762512207, 'learning_rate': 1.72e-05, 'epoch': 0.16}
{'loss': 23.4326, 'grad_norm': 2.702624559402466, 'learning_rate': 1.648e-05, 'epoch': 0.2}
{'loss': 22.548, 'grad_norm': 2.7482733726501465, 'learning_rate': 1.5680000000000002e-05, 'epoch': 0.24}
{'loss': 21.2795, 'grad_norm': 3.6650888919830322, 'learning_rate': 1.4880000000000002e-05, 'epoch': 0.28}
{'loss': 25.1103, 'grad_norm': 52.4715576171875, 'learning_rate': 1.408e-05, 'epoch': 0.32}
{'loss': 21.3941, 'grad_norm': 3.890594720840454, 'learning_rate': 1.3280000000000002e-05, 'epoch': 0.36}
{'loss': 17.7635, 'grad_norm': 3.673938274383545, 'learning_rate': 1.248e-05, 'epoch': 0.4}
{'loss': 19.4498, 'grad_norm': 3.369356155395508, 'learning_rate': 1.168e-05, 'epoch': 0.44}
{'loss': 19.9463, 'grad_norm': 3.4278981685638428, 'learning_rate': 1.0880000000000001e-05, 'epoch': 0.48}
{'loss': 23.5784, 'grad_norm': 71.29069519042969, 'learning_rate': 1.008e-05, 'epoch': 0.52}
{'loss': 23.3663, 'grad_norm': 3.648338794708252, 'learning_rate': 9.280000000000001e-06, 'epoch': 0.56}
{'loss': 19.7957, 'grad_norm': 3.4622113704681396, 'learning_rate': 8.48e-06, 'epoch': 0.6}
{'loss': 26.8844, 'grad_norm': 75.47846984863281, 'learning_rate': 7.680000000000001e-06, 'epoch': 0.64}
{'loss': 18.2903, 'grad_norm': 3.333197832107544, 'learning_rate': 6.88e-06, 'epoch': 0.68}
{'loss': 24.5627, 'grad_norm': 3.387540340423584, 'learning_rate': 6.08e-06, 'epoch': 0.72}
{'loss': 22.3925, 'grad_norm': 3.6238350868225098, 'learning_rate': 5.28e-06, 'epoch': 0.76}
{'loss': 20.3253, 'grad_norm': 3.419809579849243, 'learning_rate': 4.48e-06, 'epoch': 0.8}
{'loss': 18.7216, 'grad_norm': 65.06979370117188, 'learning_rate': 3.6800000000000003e-06, 'epoch': 0.84}
{'loss': 22.9539, 'grad_norm': 3.36755633354187, 'learning_rate': 2.88e-06, 'epoch': 0.88}
{'loss': 21.8634, 'grad_norm': 3.6643660068511963, 'learning_rate': 2.08e-06, 'epoch': 0.92}
{'loss': 22.1034, 'grad_norm': 3.2213797569274902, 'learning_rate': 1.28e-06, 'epoch': 0.96}
{'loss': 20.7484, 'grad_norm': 3.363067150115967, 'learning_rate': 4.800000000000001e-07, 'epoch': 1.0}
{'train_runtime': 723.0236, 'train_samples_per_second': 3.458, 'train_steps_per_second': 0.346, 'train_loss': 21.3652734375, 'epoch': 1.0}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [12:03<00:00,  2.89s/it]
ðŸš¨ 1 epoch åŽ LoRA adapter æ¬§æ°è·ç¦»: 11.994655
æ¨¡åž‹å·²ä¿å­˜åˆ°GAD_pou_proof/model_final

ðŸ“ˆ [Trend Verification]
Target steps            : [236, 238, 240, 242, 244, 246]
Original distances      : ['0.0404', '0.0562', '0.0699', '0.0817', '0.0921', '0.1005']
Reproduced distances    : ['0.5408', '0.6951', '1.0127', '0.9469', '1.2487', '1.3455']
ðŸ“Š Pearson correlation r: 0.9692
âœ… è¶‹åŠ¿ä¸€è‡´æ€§è‰¯å¥½ï¼ˆé«˜ç›¸å…³æ€§ï¼‰

Github

gradient ascent

{'loss': -60.475, 'grad_norm': 91.34745025634766, 'learning_rate': 1.9600000000000002e-05, 'epoch': 0.05}
{'loss': -55.8821, 'grad_norm': 104.75603485107422, 'learning_rate': 1.86e-05, 'epoch': 0.1}
{'loss': -66.8603, 'grad_norm': 219.1112060546875, 'learning_rate': 1.76e-05, 'epoch': 0.15}
{'loss': -54.8108, 'grad_norm': inf, 'learning_rate': 1.66e-05, 'epoch': 0.2}
{'loss': -65.1468, 'grad_norm': 298.4784851074219, 'learning_rate': 1.5700000000000002e-05, 'epoch': 0.25}
{'loss': -66.4283, 'grad_norm': 323.14483642578125, 'learning_rate': 1.4700000000000002e-05, 'epoch': 0.3}
{'loss': -68.1463, 'grad_norm': 353.9845886230469, 'learning_rate': 1.3700000000000003e-05, 'epoch': 0.35}
{'loss': -79.2433, 'grad_norm': 477.3984680175781, 'learning_rate': 1.2800000000000001e-05, 'epoch': 0.4}
{'loss': -76.7607, 'grad_norm': 477.5914611816406, 'learning_rate': 1.1900000000000001e-05, 'epoch': 0.45}
{'loss': -84.6623, 'grad_norm': 1059.8470458984375, 'learning_rate': 1.0900000000000002e-05, 'epoch': 0.5}
{'loss': -87.8639, 'grad_norm': 737.639892578125, 'learning_rate': 9.9e-06, 'epoch': 0.55}
{'loss': -91.2701, 'grad_norm': 770.5651245117188, 'learning_rate': 8.900000000000001e-06, 'epoch': 0.6}
{'loss': -111.176, 'grad_norm': 975.4696044921875, 'learning_rate': 7.9e-06, 'epoch': 0.65}
{'loss': -116.1027, 'grad_norm': 1215.7808837890625, 'learning_rate': 6.9e-06, 'epoch': 0.7}
{'loss': -144.4993, 'grad_norm': 1626.6431884765625, 'learning_rate': 5.9e-06, 'epoch': 0.75}
{'loss': -120.7287, 'grad_norm': 1050.4176025390625, 'learning_rate': 4.9000000000000005e-06, 'epoch': 0.8}
{'loss': -129.6881, 'grad_norm': 1278.840576171875, 'learning_rate': 3.900000000000001e-06, 'epoch': 0.85}
{'loss': -121.8858, 'grad_norm': 1355.1319580078125, 'learning_rate': 2.9e-06, 'epoch': 0.9}
{'loss': -162.868, 'grad_norm': 784.4445190429688, 'learning_rate': 1.9000000000000002e-06, 'epoch': 0.95}
{'loss': -143.9772, 'grad_norm': 1112.9339599609375, 'learning_rate': 9.000000000000001e-07, 'epoch': 1.0}
{'train_runtime': 602.2204, 'train_samples_per_second': 3.321, 'train_steps_per_second': 0.332, 'train_loss': -95.42378265380859, 'epoch': 1.0}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [10:02<00:00,  3.01s/it]
ðŸš¨ 1 epoch åŽ LoRA adapter æ¬§æ°è·ç¦»: 21.831833
æ¨¡åž‹å·²ä¿å­˜åˆ°GA_pou_proof/model_final

approximate retrain

Finetune with random labels

unlearn with adversarial samples

gradient ascent + descent




